{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download dataset\n",
    "\n",
    "Please download the dataset from: \n",
    "\n",
    "http://labs.criteo.com/2014/02/kaggle-display-advertising-challenge-dataset/\n",
    "\n",
    "This dataset is about 4.3G.\n",
    "\n",
    "Uncompress it and put 'train.txt' in the 'data' folder.\n",
    "\n",
    "Since the test set has no label, we just use train set for experiments. We split it into train set and valid set.\n",
    "\n",
    "The [origin project](https://github.com/xxxmin/ctr_Keras) preprocess by:\n",
    "\n",
    "1. fill NaN with 0.\n",
    "2. remove category features with frequence less than 10.\n",
    "\n",
    "But I find that even if you remove categories occuring less than 10 times, there are still too many possible values:\n",
    "\n",
    "```\n",
    "number of unique values:\n",
    "int_0 320\n",
    "int_1 4893\n",
    "int_2 1919\n",
    "int_3 176\n",
    "int_4 98731\n",
    "int_5 4723\n",
    "int_6 1727\n",
    "int_7 410\n",
    "int_8 3708\n",
    "int_9 9\n",
    "int_10 135\n",
    "int_11 171\n",
    "int_12 379\n",
    "str_0 1442\n",
    "str_1 553\n",
    "str_2 175780\n",
    "str_3 128508\n",
    "str_4 304\n",
    "str_5 18\n",
    "str_6 11929\n",
    "str_7 628\n",
    "str_8 3\n",
    "str_9 41223\n",
    "str_10 5159\n",
    "str_11 174834\n",
    "str_12 3174\n",
    "str_13 26\n",
    "str_14 11253\n",
    "str_15 165205\n",
    "str_16 10\n",
    "str_17 4604\n",
    "str_18 2016\n",
    "str_19 4\n",
    "str_20 172321\n",
    "str_21 17\n",
    "str_22 15\n",
    "str_23 56455\n",
    "str_24 85\n",
    "str_25 43355\n",
    "```\n",
    "\n",
    "So I use hash trick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_fname = \"data/train.txt\"\n",
    "output_train = \"data/train.csv\"\n",
    "output_valid = \"data/valid.csv\"\n",
    "\n",
    "total_lines = 45840618\n",
    "valid_size = 0.1\n",
    "col_names = ['label'] + ['int_%d' % d for d in range(13)] + [\"str_%d\" % d for d in range(26)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of unique values of each features.\n",
    "# this takes about 40 minutes to run.\n",
    "\n",
    "train_ds = tf.data.experimental.make_csv_dataset(\n",
    "    input_fname,\n",
    "    batch_size=128,\n",
    "    column_names=col_names,\n",
    "    label_name=\"label\",\n",
    "    field_delim='\\t',\n",
    "    num_epochs=1\n",
    ")\n",
    "\n",
    "cnt = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "for batch, label in tqdm(train_ds, total=total_lines//128):\n",
    "    for key, tensor in batch.items():\n",
    "        for val in tensor.numpy():\n",
    "            cnt[key][val] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique values which occur more than 10 times:\n",
      "int_0 320\n",
      "int_1 4893\n",
      "int_2 1919\n",
      "int_3 176\n",
      "int_4 98731\n",
      "int_5 4723\n",
      "int_6 1727\n",
      "int_7 410\n",
      "int_8 3708\n",
      "int_9 9\n",
      "int_10 135\n",
      "int_11 171\n",
      "int_12 379\n",
      "str_0 1442\n",
      "str_1 553\n",
      "str_2 175780\n",
      "str_3 128508\n",
      "str_4 304\n",
      "str_5 18\n",
      "str_6 11929\n",
      "str_7 628\n",
      "str_8 3\n",
      "str_9 41223\n",
      "str_10 5159\n",
      "str_11 174834\n",
      "str_12 3174\n",
      "str_13 26\n",
      "str_14 11253\n",
      "str_15 165205\n",
      "str_16 10\n",
      "str_17 4604\n",
      "str_18 2016\n",
      "str_19 4\n",
      "str_20 172321\n",
      "str_21 17\n",
      "str_22 15\n",
      "str_23 56455\n",
      "str_24 85\n",
      "str_25 43355\n"
     ]
    }
   ],
   "source": [
    "print(\"number of unique values which occur more than 10 times:\")\n",
    "for k, c in cnt.items():\n",
    "    print(k, sum(1 for v, n in c.items() if n > 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def col_process(in_f, out_f, names, key, max_cut=128):\n",
    "    sh = pd.read_csv(in_f, delimiter='\\t', names=names, usecols=(key,))\n",
    "    nunique = sh[key].nunique()\n",
    "    max_cut = min(max_cut - 1, nunique)\n",
    "\n",
    "    if key.startswith(\"str\"):\n",
    "        # using hash trick to handle string features.\n",
    "        sh[key].fillna(\"no_value\", inplace=True)\n",
    "        sh[key] = pd.factorize(sh[key])[0]\n",
    "        sh[key] = sh[key] % max_cut\n",
    "    else:\n",
    "        # split buckets for number featuers.\n",
    "        sh[key] = pd.cut(sh[key], max_cut, labels=range(max_cut)).cat.codes\n",
    "        sh[key].replace(-1, max_cut, inplace=True)\n",
    "\n",
    "    sh.to_csv(out_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing key: label\n",
      "processing key: int_0\n",
      "processing key: int_1\n",
      "processing key: int_2\n",
      "processing key: int_3\n",
      "processing key: int_4\n",
      "processing key: int_5\n",
      "processing key: int_6\n",
      "processing key: int_7\n",
      "processing key: int_8\n",
      "processing key: int_9\n",
      "processing key: int_10\n",
      "processing key: int_11\n",
      "processing key: int_12\n",
      "processing key: str_0\n",
      "processing key: str_1\n",
      "processing key: str_2\n",
      "processing key: str_3\n",
      "processing key: str_4\n",
      "processing key: str_5\n",
      "processing key: str_6\n",
      "processing key: str_7\n",
      "processing key: str_8\n",
      "processing key: str_9\n",
      "processing key: str_10\n",
      "processing key: str_11\n",
      "processing key: str_12\n",
      "processing key: str_13\n",
      "processing key: str_14\n",
      "processing key: str_15\n",
      "processing key: str_16\n",
      "processing key: str_17\n",
      "processing key: str_18\n",
      "processing key: str_19\n",
      "processing key: str_20\n",
      "processing key: str_21\n",
      "processing key: str_22\n",
      "processing key: str_23\n",
      "processing key: str_24\n",
      "processing key: str_25\n"
     ]
    }
   ],
   "source": [
    "# this cell takes about 40 minutes to run.\n",
    "col_fname = [input_fname + \"_%s.csv\" % key for key in col_names]\n",
    "\n",
    "for key, out_f in zip(col_names, col_fname):\n",
    "    print(\"processing key:\", key)\n",
    "    col_process(input_fname, out_f, col_names, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_lines = int(total_lines * valid_size)\n",
    "train_lines = total_lines - valid_lines\n",
    "\n",
    "def merge_cols(col_fname, output_train, output_valid, lim=100):\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    col_f = [open(f) for f in col_fname]\n",
    "    train_f = open(output_train, 'w')\n",
    "    valid_f = open(output_valid, 'w')\n",
    "    \n",
    "    for idx, lines in tqdm(enumerate(zip(*col_f))):\n",
    "        keys = [s.strip().split(\",\")[1] for s in lines]\n",
    "        merged_s = \",\".join(keys) + \"\\n\"\n",
    "        if idx == 0:\n",
    "            train_f.write(merged_s)\n",
    "            valid_f.write(merged_s)\n",
    "        elif idx < train_lines:\n",
    "            train_f.write(merged_s)\n",
    "        else:\n",
    "            valid_f.write(merged_s)\n",
    "        \n",
    "        if lim != -1 and idx > lim:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44795960it [14:52, 41576.45it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "merge_cols(col_fname, output_train, output_valid, lim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm data/train.txt_*.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label,int_0,int_1,int_2,int_3,int_4,int_5,int_6,int_7,int_8,int_9,int_10,int_11,int_12,str_0,str_1,str_2,str_3,str_4,str_5,str_6,str_7,str_8,str_9,str_10,str_11,str_12,str_13,str_14,str_15,str_16,str_17,str_18,str_19,str_20,str_21,str_22,str_23,str_24,str_25\n",
      "0,0,0,0,0,0,0,0,0,0,1,1,127,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
      "0,0,0,0,0,0,0,0,0,0,1,0,127,0,0,1,1,1,0,1,1,1,0,1,1,1,1,1,1,1,1,1,0,1,1,0,0,1,0,1\n",
      "0,0,0,0,1,0,0,0,0,1,1,1,0,0,1,2,2,2,0,0,2,1,0,2,2,2,2,2,2,2,2,2,1,2,2,1,0,2,1,2\n",
      "0,127,0,127,127,0,127,0,0,0,12,0,127,127,0,3,3,3,0,1,3,1,0,3,3,3,3,2,3,3,3,3,1,2,3,0,0,3,1,2\n",
      "0,0,0,127,0,0,0,0,0,0,1,0,127,0,2,4,4,4,0,2,4,1,0,4,4,4,4,0,4,4,3,4,1,2,4,0,1,4,1,2\n",
      "0,127,0,127,127,0,127,0,0,0,12,0,127,127,3,5,5,5,1,3,5,1,0,2,5,5,5,2,5,5,4,5,1,2,5,2,2,5,1,2\n",
      "0,127,0,0,127,0,127,0,0,0,12,0,127,127,4,6,6,6,1,1,6,1,0,2,6,6,6,2,6,6,4,6,1,2,6,0,3,6,1,2\n",
      "1,0,0,0,0,0,0,0,0,0,1,0,127,0,0,3,7,7,2,2,7,0,0,5,7,7,7,2,3,7,0,3,1,2,7,0,1,3,1,2\n",
      "0,127,0,0,1,0,0,0,0,0,12,0,127,0,3,7,8,2,0,0,8,1,0,6,8,8,8,1,7,8,0,7,1,2,8,0,1,2,1,2\n"
     ]
    }
   ],
   "source": [
    "!head data/train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label,int_0,int_1,int_2,int_3,int_4,int_5,int_6,int_7,int_8,int_9,int_10,int_11,int_12,str_0,str_1,str_2,str_3,str_4,str_5,str_6,str_7,str_8,str_9,str_10,str_11,str_12,str_13,str_14,str_15,str_16,str_17,str_18,str_19,str_20,str_21,str_22,str_23,str_24,str_25\n",
      "0,0,0,0,0,0,0,0,0,0,0,0,127,0,0,61,124,105,5,0,117,1,0,117,90,36,124,2,43,64,0,91,22,0,14,1,1,21,29,9\n",
      "1,127,0,0,0,0,0,0,0,0,12,2,127,0,7,0,48,12,3,1,85,5,0,96,85,80,49,2,5,25,1,0,0,0,92,0,1,19,0,25\n",
      "0,127,0,127,0,0,0,0,0,0,12,0,127,0,0,21,8,2,0,0,84,1,0,39,79,8,77,2,28,8,0,96,1,2,8,1,1,2,1,2\n",
      "0,0,0,0,1,0,0,0,0,0,2,4,0,0,67,66,121,109,0,5,61,1,0,26,16,121,110,2,41,118,2,126,1,2,121,0,9,89,1,2\n",
      "0,0,0,127,0,0,0,0,0,1,1,15,0,0,7,34,7,126,3,5,123,6,0,108,11,79,11,0,51,79,2,46,0,0,59,0,1,21,6,9\n",
      "1,0,0,0,0,0,0,0,0,1,1,3,127,0,7,9,33,9,0,0,62,1,0,111,123,25,108,2,97,63,0,118,0,1,123,0,6,2,2,6\n",
      "0,127,0,127,4,0,127,0,0,0,12,0,127,0,3,109,43,111,0,1,42,0,0,57,30,77,22,1,109,78,0,42,1,2,96,1,4,30,1,2\n",
      "0,127,0,127,0,0,127,0,0,0,12,0,127,0,3,4,117,4,2,1,2,0,0,61,126,31,91,2,67,4,3,14,1,2,4,0,3,4,1,2\n",
      "0,0,0,0,0,0,0,0,0,0,1,0,127,0,7,77,47,43,3,0,36,1,0,75,26,47,18,0,23,46,5,115,0,1,47,0,8,2,4,78\n"
     ]
    }
   ],
   "source": [
    "!head data/valid.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from tensorflow.keras import callbacks\n",
    "import tensorflow as tf\n",
    "print(tf.config.get_visible_devices(\"GPU\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CUT = 128\n",
    "TRAIN_FNAME = \"data/train.csv\"\n",
    "VALID_FNAME = \"data/valid.csv\"\n",
    "col_names = ['label'] + ['int_%d' % d for d in range(13)] + [\"str_%d\" % d for d in range(26)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_csv(fname, n=40):\n",
    "#     with open(fname) as f:\n",
    "#         for li in f:\n",
    "#             if li.count(\",\") != n - 1:\n",
    "#                 print(li)\n",
    "#                 print(li.count(\",\"))\n",
    "#                 break\n",
    "\n",
    "# check_csv(TRAIN_FNAME)\n",
    "# check_csv(VALID_FNAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(filename, batch_size=256):\n",
    "    return tf.data.experimental.make_csv_dataset(\n",
    "        filename,\n",
    "        batch_size=batch_size,\n",
    "        column_names=col_names,\n",
    "        label_name=\"label\",\n",
    "        num_epochs=1\n",
    "    )\n",
    "\n",
    "train_ds = read_csv(TRAIN_FNAME)\n",
    "valid_ds = read_csv(VALID_FNAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, epochs=1, checkpoint_fname=None):\n",
    "    callback_list = []\n",
    "    callback_list.append(callbacks.EarlyStopping(monitor=\"val_loss\", patience=2))\n",
    "    \n",
    "    if checkpoint_fname:\n",
    "        ck = callbacks.ModelCheckpoint(\"checkpoints/fnn.h5\",\n",
    "            save_weights_only=True, verbose=1, save_best_only=True)\n",
    "        callback_list.append(ck)\n",
    "    \n",
    "    model.fit(train_ds, epochs=epochs, validation_data=valid_ds, callbacks=callback_list)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 123040/Unknown - 1806s 15ms/step - loss: 0.4964 - acc: 0.7656"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 133021/Unknown - 1953s 15ms/step - loss: 0.4967 - acc: 0.7653"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161159/161159 [==============================] - 2582s 16ms/step - loss: 0.4967 - acc: 0.7652 - val_loss: 0.5051 - val_acc: 0.7610\n"
     ]
    }
   ],
   "source": [
    "from models.lr import make_lr_model\n",
    "lr_model = train_model(make_lr_model(col_names[1:], [MAX_CUT] * (len(col_names) - 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 57600 samples, validate on 6400 samples\n",
      "Epoch 1/2\n",
      "57600/57600 [==============================] - 10s 167us/sample - loss: 0.5226 - acc: 0.7491 - val_loss: 0.5447 - val_acc: 0.7428\n",
      "Epoch 2/2\n",
      "57600/57600 [==============================] - 6s 96us/sample - loss: 0.3630 - acc: 0.8420 - val_loss: 0.5251 - val_acc: 0.7480\n"
     ]
    }
   ],
   "source": [
    "from models.fnn import make_fnn_model\n",
    "fnn_model = train_model(make_fnn_model(cols, val_nums))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NFM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 57600 samples, validate on 6400 samples\n",
      "Epoch 1/2\n",
      "57600/57600 [==============================] - 17s 303us/sample - loss: 0.5215 - acc: 0.7486 - val_loss: 0.5780 - val_acc: 0.7428\n",
      "Epoch 2/2\n",
      "57600/57600 [==============================] - 10s 179us/sample - loss: 0.3560 - acc: 0.8435 - val_loss: 0.5516 - val_acc: 0.7425\n"
     ]
    }
   ],
   "source": [
    "from models.nfm import make_nfm_model\n",
    "nfm_model = make_nfm_model(cols, val_nums, interact=\"multiply\", merge=\"add\")\n",
    "nfm_model = train_model(nfm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 57600 samples, validate on 6400 samples\n",
      "Epoch 1/2\n",
      "57600/57600 [==============================] - 36s 621us/sample - loss: 0.5430 - acc: 0.7287 - val_loss: 0.5859 - val_acc: 0.7428\n",
      "Epoch 2/2\n",
      "57600/57600 [==============================] - 9s 152us/sample - loss: 0.3523 - acc: 0.8488 - val_loss: 0.5398 - val_acc: 0.7436\n"
     ]
    }
   ],
   "source": [
    "nfm_model = make_nfm_model(cols, val_nums, interact=\"dot\", merge=\"concat\")\n",
    "nfm_model = train_model(nfm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 57600 samples, validate on 6400 samples\n",
      "Epoch 1/2\n",
      "57600/57600 [==============================] - 18s 315us/sample - loss: 0.5222 - acc: 0.7447 - val_loss: 0.5993 - val_acc: 0.7428\n",
      "Epoch 2/2\n",
      "57600/57600 [==============================] - 10s 177us/sample - loss: 0.3553 - acc: 0.8444 - val_loss: 0.5425 - val_acc: 0.7430\n"
     ]
    }
   ],
   "source": [
    "nfm_model = make_nfm_model(cols, val_nums, interact=\"multiply\", merge=\"concat\")\n",
    "nfm_model = train_model(nfm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
